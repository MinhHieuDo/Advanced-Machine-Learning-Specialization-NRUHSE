{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week5_practice_reinforce_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_unebAk7lE1f",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF2PMe5ElE1h",
        "colab_type": "code",
        "outputId": "984634cc-eb46-42d4-9bfb-9dbc1e5edf6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np3dtwdAlE1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjZ9ovaglE10",
        "colab_type": "text"
      },
      "source": [
        "A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoRXOfkhlE11",
        "colab_type": "code",
        "outputId": "10fcc11f-bf24-448d-a893-c0bc6f44a642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb570cd9710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAATGUlEQVR4nO3df6zddZ3n8eerPyggrrRyrd22TBnthsXJWMgVMLoJg3EG2c3CJK6BnSBxSDqTYKKJmVmYTXY0WcxM3BHX7EC2E1AcXZEdRRqCq7WSTPxDoGCthcJQtA7ttLQgvypQbPveP+63eCht77m/evq55/lITs73+/5+v+f7/oTTF9/7ud9zT6oKSVI75gy6AUnSxBjcktQYg1uSGmNwS1JjDG5JaozBLUmNmbHgTnJJkseSbE1y3UydR5KGTWbiPu4kc4F/Aj4IbAceAK6sqkem/WSSNGRm6or7fGBrVf2sql4Fbgcum6FzSdJQmTdDr7sUeLJnfTtwwdF2PuOMM2rFihUz1IoktWfbtm08/fTTOdK2mQrucSVZDawGOPPMM9mwYcOgWpGkE87o6OhRt83UVMkOYHnP+rKu9pqqWlNVo1U1OjIyMkNtSNLsM1PB/QCwMslZSU4CrgDWztC5JGmozMhUSVXtT/Jx4LvAXODWqnp4Js4lScNmxua4q+oe4J6Zen1JGlZ+clKSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmOm9NVlSbYBLwIHgP1VNZpkEfANYAWwDfhIVT07tTYlSYdMxxX371XVqqoa7davA9ZX1UpgfbcuSZomMzFVchlwW7d8G3D5DJxDkobWVIO7gO8leTDJ6q62uKp2dsu7gMVTPIckqceU5riB91fVjiRvA9YlebR3Y1VVkjrSgV3QrwY488wzp9iGJA2PKV1xV9WO7nk3cCdwPvBUkiUA3fPuoxy7pqpGq2p0ZGRkKm1I0lCZdHAneVOSNx9aBn4f2AysBa7udrsauGuqTUqSfmMqUyWLgTuTHHqd/1NV/y/JA8AdSa4BfgF8ZOptSpIOmXRwV9XPgHcfof4M8IGpNCVJOjo/OSlJjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1ZtzgTnJrkt1JNvfUFiVZl+Tx7nlhV0+SLybZmmRTkvNmsnlJGkb9XHF/GbjksNp1wPqqWgms79YBPgSs7B6rgZunp01J0iHjBndV/SPwy8PKlwG3dcu3AZf31L9SY34EnJ5kyXQ1K0ma/Bz34qra2S3vAhZ3y0uBJ3v2297V3iDJ6iQbkmzYs2fPJNuQpOEz5V9OVlUBNYnj1lTVaFWNjoyMTLUNSRoakw3upw5NgXTPu7v6DmB5z37LupokaZpMNrjXAld3y1cDd/XUP9rdXXIh8HzPlIokaRrMG2+HJF8HLgLOSLId+Evgr4A7klwD/AL4SLf7PcClwFbgJeBjM9CzJA21cYO7qq48yqYPHGHfAq6dalOSpKPzk5OS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhozbnAnuTXJ7iSbe2qfTrIjycbucWnPtuuTbE3yWJI/mKnGJWlY9XPF/WXgkiPUb6yqVd3jHoAk5wBXAO/qjrkpydzpalaS1EdwV9U/Ar/s8/UuA26vqn1V9XPGvu39/Cn0J0k6zFTmuD+eZFM3lbKwqy0FnuzZZ3tXe4Mkq5NsSLJhz549U2hDkobLZIP7ZuAdwCpgJ/A3E32BqlpTVaNVNToyMjLJNiRp+EwquKvqqao6UFUHgb/jN9MhO4DlPbsu62qSpGkyqeBOsqRn9Q+BQ3ecrAWuSLIgyVnASuD+qbUoSeo1b7wdknwduAg4I8l24C+Bi5KsAgrYBvwJQFU9nOQO4BFgP3BtVR2YmdYlaTiNG9xVdeURyrccY/8bgBum0pQk6ej85KQkNcbglqTGGNyS1BiDW5IaY3BLUmPGvatEms32v7KXl57Zzpx5J/Gmt51FkkG3JI3L4NZQ27vrCZ743k3MmXcSp739nWPFzOG3/t0fcdJpiwbbnHQUBrcEHNz/Ki9sf2RsJeHg/lcH25B0DM5xS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbQ6uq2Ltr6xvqif8sdGLzHarhVcXz/7zpDeXTzzqPk958xgAakvpjcEuHmTv/ZObM9UPFOnGNG9xJlie5N8kjSR5O8omuvijJuiSPd88Lu3qSfDHJ1iSbkpw304OQpGHSzxX3fuBTVXUOcCFwbZJzgOuA9VW1EljfrQN8iLFvd18JrAZunvauJWmIjRvcVbWzqh7qll8EtgBLgcuA27rdbgMu75YvA75SY34EnJ5kybR3LklDakJz3ElWAOcC9wGLq2pnt2kXsLhbXgo82XPY9q52+GutTrIhyYY9e/ZMsG1JGl59B3eS04BvAp+sqhd6t1VVATWRE1fVmqoararRkZGRiRwqSUOtr+BOMp+x0P5aVX2rKz91aAqke97d1XcAy3sOX9bVJEnToJ+7SgLcAmypqs/3bFoLXN0tXw3c1VP/aHd3yYXA8z1TKpKkKernZtX3AVcBP02ysav9BfBXwB1JrgF+AXyk23YPcCmwFXgJ+Ni0dixJQ27c4K6qHwJH+wbVDxxh/wKunWJfkqSj8JOTktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5Ia08+XBS9Pcm+SR5I8nOQTXf3TSXYk2dg9Lu055vokW5M8luQPZnIAkjRs+vmy4P3Ap6rqoSRvBh5Msq7bdmNV/Y/enZOcA1wBvAv418D3k/ybqjownY1L0rAa94q7qnZW1UPd8ovAFmDpMQ65DLi9qvZV1c8Z+7b386ejWUnSBOe4k6wAzgXu60ofT7Ipya1JFna1pcCTPYdt59hBL0magL6DO8lpwDeBT1bVC8DNwDuAVcBO4G8mcuIkq5NsSLJhz549EzlUmhYv/sujvPqr515Xy5x5LHzH6IA6kvrTV3Anmc9YaH+tqr4FUFVPVdWBqjoI/B2/mQ7ZASzvOXxZV3udqlpTVaNVNToyMjKVMUiTsu/FZzj461deV8ucOZyycMmAOpL6089dJQFuAbZU1ed76r3v7j8ENnfLa4ErkixIchawErh/+lqWpOHWz10l7wOuAn6aZGNX+wvgyiSrgAK2AX8CUFUPJ7kDeISxO1Ku9Y4SSZo+4wZ3Vf0QyBE23XOMY24AbphCX5Kko/CTk5LUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY3p58+6Ss144IEH+OxnP9vXvucuP5l//zv/6nW1V17Zx8f++I/51b6D4x6/aNEibrrpJhYsWDCpXqXJMrg1qzz11FN8+9vf7m/n95/NJe+6iP0HDwVvceDAXr7zne/wyxdeHvfwJUuWcOCAf2pex5/BraFVzOHRFy7gn18+G4C5+TVnn/K9AXcljc/g1tD6l5ffwbaX3kV1v+o5UPP5xUvncLD8Z6ETm7+c1NA6UHNfC+1D9uxbzq8Pzh9QR1J/+vmy4JOT3J/kJ0keTvKZrn5WkvuSbE3yjSQndfUF3frWbvuKmR2CNDkL5rzMHPa/rrb0lK2cNGffgDqS+tPPFfc+4OKqejewCrgkyYXAXwM3VtU7gWeBa7r9rwGe7eo3dvtJJ5w31eOc+tJ3efrpbWT/Ht4yfzcrTt1MMv4dJdIg9fNlwQXs7Vbnd48CLgb+c1e/Dfg0cDNwWbcM8A/A/0qS7nWkE8b3H/wZ6x/6NBAu+LdLWfLW07j3wEF+9fKrg25NOqa+fguTZC7wIPBO4G+BJ4DnqurQz5nbgaXd8lLgSYCq2p/keeCtwNNHe/1du3bxuc99blIDkHpt2bJlQvuPXU8UP3rkyQmfa+/evXzhC19g/nznxDX9du3addRtfQV3VR0AViU5HbgTOHuqTSVZDawGWLp0KVddddVUX1Ji3bp1fOlLXzou5zr11FO58sorOeWUU47L+TRcvvrVrx5124Tue6qq55LcC7wXOD3JvO6qexmwo9ttB7Ac2J5kHvAW4JkjvNYaYA3A6Ohovf3tb59IK9IRLVy48Lida86cOSxevJhTTz31uJ1Tw+NYP8n1c1fJSHelTZJTgA8CW4B7gQ93u10N3NUtr+3W6bb/wPltSZo+/VxxLwFu6+a55wB3VNXdSR4Bbk/y34EfA7d0+98C/H2SrcAvgStmoG9JGlr93FWyCTj3CPWfAecfof4K8J+mpTtJ0hv4yUlJaozBLUmN8a/paFZZvHgxl19++XE516JFi5g7d+5xOZfUy+DWrPKe97yHO++8c9BtSDPKqRJJaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1Jh+viz45CT3J/lJkoeTfKarfznJz5Ns7B6runqSfDHJ1iSbkpw304OQpGHSz9/j3gdcXFV7k8wHfpjkO922P6uqfzhs/w8BK7vHBcDN3bMkaRqMe8VdY/Z2q/O7Rx3jkMuAr3TH/Qg4PcmSqbcqSYI+57iTzE2yEdgNrKuq+7pNN3TTITcmWdDVlgJP9hy+vatJkqZBX8FdVQeqahWwDDg/ye8A1wNnA+8BFgH/ZSInTrI6yYYkG/bs2TPBtiVpeE3orpKqeg64F7ikqnZ20yH7gC8B53e77QCW9xy2rKsd/lprqmq0qkZHRkYm170kDaF+7ioZSXJ6t3wK8EHg0UPz1kkCXA5s7g5ZC3y0u7vkQuD5qto5I91L0hDq566SJcBtSeYyFvR3VNXdSX6QZAQIsBH4027/e4BLga3AS8DHpr9tSRpe4wZ3VW0Czj1C/eKj7F/AtVNvTZJ0JH5yUpIaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNSZVNegeSPIi8Nig+5ghZwBPD7qJGTBbxwWzd2yOqy2/VVUjR9ow73h3chSPVdXooJuYCUk2zMaxzdZxwewdm+OaPZwqkaTGGNyS1JgTJbjXDLqBGTRbxzZbxwWzd2yOa5Y4IX45KUnq34lyxS1J6tPAgzvJJUkeS7I1yXWD7meiktyaZHeSzT21RUnWJXm8e17Y1ZPki91YNyU5b3CdH1uS5UnuTfJIkoeTfKKrNz22JCcnuT/JT7pxfaarn5Xkvq7/byQ5qasv6Na3dttXDLL/8SSZm+THSe7u1mfLuLYl+WmSjUk2dLWm34tTMdDgTjIX+FvgQ8A5wJVJzhlkT5PwZeCSw2rXAeuraiWwvluHsXGu7B6rgZuPU4+TsR/4VFWdA1wIXNv9t2l9bPuAi6vq3cAq4JIkFwJ/DdxYVe8EngWu6fa/Bni2q9/Y7Xci+wSwpWd9towL4PeqalXPrX+tvxcnr6oG9gDeC3y3Z/164PpB9jTJcawANvesPwYs6ZaXMHafOsD/Bq480n4n+gO4C/jgbBobcCrwEHABYx/gmNfVX3tfAt8F3tstz+v2y6B7P8p4ljEWYBcDdwOZDePqetwGnHFYbda8Fyf6GPRUyVLgyZ717V2tdYurame3vAtY3C03Od7ux+hzgfuYBWPrphM2AruBdcATwHNVtb/bpbf318bVbX8eeOvx7bhvXwD+HDjYrb+V2TEugAK+l+TBJKu7WvPvxck6UT45OWtVVSVp9tadJKcB3wQ+WVUvJHltW6tjq6oDwKokpwN3AmcPuKUpS/IfgN1V9WCSiwbdzwx4f1XtSPI2YF2SR3s3tvpenKxBX3HvAJb3rC/raq17KskSgO55d1dvarxJ5jMW2l+rqm915VkxNoCqeg64l7EphNOTHLqQ6e39tXF1298CPHOcW+3H+4D/mGQbcDtj0yX/k/bHBUBV7eiedzP2P9vzmUXvxYkadHA/AKzsfvN9EnAFsHbAPU2HtcDV3fLVjM0PH6p/tPut94XA8z0/6p1QMnZpfQuwpao+37Op6bElGemutElyCmPz9lsYC/APd7sdPq5D4/0w8IPqJk5PJFV1fVUtq6oVjP07+kFV/RGNjwsgyZuSvPnQMvD7wGYafy9OyaAn2YFLgX9ibJ7xvw66n0n0/3VgJ/BrxubSrmFsrnA98DjwfWBRt28Yu4vmCeCnwOig+z/GuN7P2LziJmBj97i09bEBvwv8uBvXZuC/dfXfBu4HtgL/F1jQ1U/u1rd223970GPoY4wXAXfPlnF1Y/hJ93j4UE60/l6cysNPTkpSYwY9VSJJmiCDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4Jakxvx/I7JxPrwTgmsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcV2CrSElE17",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5LxXmoKlE19",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXw8NFeblE19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjkHd6TEQqYS",
        "colab_type": "code",
        "outputId": "f9027770-853a-47cd-e06b-4b7c0e98cbf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "state_dim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBqhqw5flE2E",
        "colab_type": "code",
        "outputId": "a0655c5b-adad-4aab-d32d-a208e2da2a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(state_dim[0],64), nn.ReLU(),\n",
        "  nn.Linear(64,n_actions)\n",
        ")\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=64, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR4DwUDQlE2K",
        "colab_type": "text"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHD7safSlE2M",
        "colab_type": "text"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeKzbGzulE2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "      logits = model(states)\n",
        "      policy =nn.functional.softmax(logits, -1)\n",
        "    return policy.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaiNxQCalE2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2YhibGlE2W",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywPetroOlE2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axCfSoBelE2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C93RnFz_lE2h",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DipQZHR_lE2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    cumulative_rewards = np.array(rewards).astype(np.float32)\n",
        "    for i in reversed(range(len(rewards)-1)):\n",
        "      cumulative_rewards[i] += gamma*cumulative_rewards[i+1]\n",
        "    return cumulative_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS2lGEFQlE2m",
        "colab_type": "code",
        "outputId": "b9c799c1-145e-43da-ff0e-d02813180442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bjHsPAKlE2s",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohvqz0milE2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqHkyINClE2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = - torch.sum(probs*log_probs)\n",
        "    J = torch.mean(log_probs_for_actions*cumulative_returns)\n",
        "    \n",
        "    loss = -(J+ entropy_coef*entropy)\n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCJQgjrQlE21",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1qbgtk2lE22",
        "colab_type": "code",
        "outputId": "398a894b-d43e-497e-f03c-dd27ca707c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:19.530\n",
            "mean reward:25.140\n",
            "mean reward:32.660\n",
            "mean reward:35.860\n",
            "mean reward:42.440\n",
            "mean reward:44.210\n",
            "mean reward:55.350\n",
            "mean reward:78.170\n",
            "mean reward:100.600\n",
            "mean reward:138.540\n",
            "mean reward:163.890\n",
            "mean reward:128.800\n",
            "mean reward:162.910\n",
            "mean reward:116.660\n",
            "mean reward:214.380\n",
            "mean reward:212.590\n",
            "mean reward:203.920\n",
            "mean reward:188.100\n",
            "mean reward:160.760\n",
            "mean reward:240.240\n",
            "mean reward:331.160\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L1_drzvlE26",
        "colab_type": "text"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArTFaVpelE27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02jZrbk0lE2_",
        "colab_type": "code",
        "outputId": "49b40bd2-5317-416a-8975-f1ab9632fb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.0.3291.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezxBoPSAlE3D",
        "colab_type": "code",
        "outputId": "2d52b699-40bf-4b88-a3b1-269bc9221851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, '', '')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0s37_JzlE3H",
        "colab_type": "text"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}